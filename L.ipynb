{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWX28F6z8bs4Kl+JZmg3aw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/layan121213/AASQ/blob/main/L.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZbCyJw25wWe",
        "outputId": "d1ce5351-e35e-4e8f-d0a1-582f00c64098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pprint\n",
        "from gensim.models import word2vec , KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import nltk \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "import gensim\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from F1 import evaluationMetric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "qxhpLjZXB0rE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BNF50kZB98r",
        "outputId": "92695f23-6ffe-41bc-a525-33c5f20c6a39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin\" , binary=True)"
      ],
      "metadata": {
        "id": "4751nQGdCCxa"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gingerit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w8EqEoYlCGFY",
        "outputId": "b1411741-0737-4ed3-a3dd-a5c3a28b409e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gingerit in /usr/local/lib/python3.7/dist-packages (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.25.1 in /usr/local/lib/python3.7/dist-packages (from gingerit) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2021.10.8)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (1.24.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import cloudscraper\n",
        "# session = cloudscraper.create_scraper()"
      ],
      "metadata": {
        "id": "VxOk36T0CTN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cloudscraper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ba1UfJDhCI05",
        "outputId": "339dc3f0-4629-48e8-9190-51386a13489e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cloudscraper\n",
            "  Downloading cloudscraper-1.2.60-py2.py3-none-any.whl (97 kB)\n",
            "\u001b[?25l\r\u001b[K     |███▍                            | 10 kB 20.0 MB/s eta 0:00:01\r\u001b[K     |██████▊                         | 20 kB 24.0 MB/s eta 0:00:01\r\u001b[K     |██████████▏                     | 30 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████▌                  | 40 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 51 kB 7.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 61 kB 9.1 MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 71 kB 10.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████     | 81 kB 9.5 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▍ | 92 kB 10.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 97 kB 4.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.4.7 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (3.0.8)\n",
            "Collecting requests-toolbelt>=0.9.1\n",
            "  Downloading requests_toolbelt-0.9.1-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[K     |████████████████████████████████| 54 kB 909 kB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.9.2 in /usr/local/lib/python3.7/dist-packages (from cloudscraper) (2.27.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2.10)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (1.24.3)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2.0.12)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.9.2->cloudscraper) (2021.10.8)\n",
            "Installing collected packages: requests-toolbelt, cloudscraper\n",
            "Successfully installed cloudscraper-1.2.60 requests-toolbelt-0.9.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stems = [stemmer.stem(word) for word in word_tokens]\n",
        "    return ' '.join(stems)\n",
        "\n",
        "def stemming(text):\n",
        "    \n",
        "    snowball = SnowballStemmer(language='english')\n",
        "    \n",
        "    list=[]\n",
        "    for token in word_tokenize(text):\n",
        "        list.append(snowball.stem(token))\n",
        "    \n",
        "    return ' '.join(list)\n",
        "\n",
        "def preprocessing(t):\n",
        "    # print(word_tokenize(t))\n",
        "    t = str(t).lower()  # lowercasing\n",
        "    no_punc = t.translate(str.maketrans(\n",
        "        '', '', string.punctuation))  # punc removal\n",
        "    # stem = stem_words(no_punc)\n",
        "    # print(\"stem\",stem)\n",
        "    no_stopWords = stopwords.words('english')  # stop words removal\n",
        "    text_tokens = word_tokenize(no_punc)\n",
        "    not_stopword = [word for word in text_tokens if not word in no_stopWords]\n",
        "    # print(\"not_stopword\",not_stopword)\n",
        "    return not_stopword\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "    cleaned = ' '.join([str(i) for i in text]).replace('[\\'', ' ').replace('\\']', '')\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def listToString(s): \n",
        "    str1 = \"\" \n",
        "    for ele in s: \n",
        "        str1 += ele  \n",
        "    return str1 \n",
        "\n",
        "w2v_vocab = set(model.vocab)\n",
        "index2word_set = set(model.wv.index2word)\n",
        "\n",
        "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
        "    words = sentence.split()\n",
        "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            n_words += 1\n",
        "            feature_vec = np.add(feature_vec, model[word])\n",
        "    if (n_words > 0):\n",
        "        feature_vec = np.divide(feature_vec, n_words)\n",
        "    return feature_vec"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ON93KLTCKBu",
        "outputId": "32bbe8d0-a3ab-4dc3-f221-1933deeed506"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:52: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "# from spellchecker import SpellChecker\n",
        "import math\n",
        "# from autocorrect import Speller\n",
        "from scipy.sparse import csr_matrix\n",
        "from numpy import count_nonzero\n",
        "import cloudscraper\n",
        "from gingerit.gingerit import GingerIt\n",
        "\n",
        "parser = GingerIt()\n",
        "# check = Speller(lang='en')\n",
        "# spell = SpellChecker()\n",
        "# check = Speller(lang='en')\n",
        "text = \"An algoriithm is a sequence of steps or a plan to complete an action or do something.\"\n",
        "ModwlA= parser.parse(text)['result']\n",
        "toString2 = clean(preprocessing(ModwlA))\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/test.xlsx\", usecols=\"A\", index_col=None)\n",
        "a = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/test.xlsx\", usecols=\"B\", index_col=None)\n",
        "count =0\n",
        "jacc = 0\n",
        "cos = 0.0\n",
        "div = 0\n",
        "list = []  \n",
        "val = []\n",
        "\n",
        "\n",
        "for rowIndex, row in df.iterrows(): \n",
        "    for value in row.items():\n",
        "        # print(\"res>>>>>\",value[1])\n",
        "        res=parser.parse(value[1])['result']\n",
        "        # print(\"gingerit:\",res)\n",
        "        toString = clean(preprocessing(res))\n",
        "        # print(\"Pre>>>>>\",toString)\n",
        "        s1_afv=avg_feature_vector(toString2, model=model, num_features=300, index2word_set=index2word_set)\n",
        "        S = csr_matrix(s1_afv)\n",
        "        B = S.todense()\n",
        "        s2_afv = avg_feature_vector(toString, model=model, num_features=300, index2word_set=index2word_set)\n",
        "        L = csr_matrix(s2_afv)\n",
        "        G = L.todense()\n",
        "        sim = 1 - spatial.distance.cosine(B, G)\n",
        "        if( math.isnan(sim)):\n",
        "          print(toString)\n",
        "          sim=0.0\n",
        "          print(sim)\n",
        "          val.append(sim)\n",
        "        else:\n",
        "          print(toString)\n",
        "          print(sim)\n",
        "          val.append(sim)\n",
        "\n",
        "        # cos += sim\n",
        "        # print(toString)\n",
        "        # print(sim)\n",
        "        # print(\"cos\",count)\n",
        "        \n",
        "predicted = [float(x) for x in val]\n",
        "actual = [int(x) for x in a.values] \n",
        "f1 = evaluationMetric(actual,predicted)\n",
        "print(f1)\n",
        "# print(index2word_set)\n",
        "# sparsity = 1.0 - count_nonzero(s1_afv)/ s1_afv.size\n",
        "# print(\"s1_afv\", sparsity)\n",
        "# sparsity2 = 1.0 - count_nonzero(s2_afv)/ s2_afv.size\n",
        "# print(\"s2_afv\", sparsity2)\n",
        "# print(\"\\njac average: \"+str(jacc/count))\n",
        "# print(\"average: \"+str(cos/count))\n",
        "# print(\"div average: \"+str(div/count))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OCamHvJJCV6Q",
        "outputId": "5821f52d-9804-4215-be1d-5627eb38da9c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in float_scalars\n",
            "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "uml\n",
            "0.0\n",
            "sequence instructions perform certain task\n",
            "0.5849913358688354\n",
            "method solve problem\n",
            "0.42462411522865295\n",
            "defined steps solve problem\n",
            "0.5190767645835876\n",
            "way solve problem\n",
            "0.3802807331085205\n",
            "steps solve problems\n",
            "0.4799351990222931\n",
            "plan program\n",
            "0.4915144741535187\n",
            "steps\n",
            "0.5903866291046143\n",
            "plan steps\n",
            "0.6936665773391724\n",
            "step step something\n",
            "0.5434535145759583\n",
            "set mathematical logical steps solve problem\n",
            "0.5785333514213562\n",
            "function\n",
            "0.2761096954345703\n",
            "sequence steps complete certain task\n",
            "0.7689512968063354\n",
            "steps coding steps solve problem\n",
            "0.6427432298660278\n",
            "method solving problem\n",
            "0.40186581015586853\n",
            "redirects subfield computer science see analysis algorithms\n",
            "0.5027896165847778\n",
            "way expression programmes\n",
            "0.3748820424079895\n",
            "serten step solve problem\n",
            "0.43450164794921875\n",
            "problem solving method\n",
            "0.40186581015586853\n",
            "sequence steps solve problem\n",
            "0.6827989220619202\n",
            "progress\n",
            "0.2666063904762268\n",
            "sequence steps something\n",
            "0.8232182860374451\n",
            "way find solution problem programming\n",
            "0.4651833176612854\n",
            "steps solve problems\n",
            "0.4799351990222931\n",
            "write code solve problem\n",
            "0.3932150602340698\n",
            "plan way solve problems\n",
            "0.4946111738681793\n"
          ]
        }
      ]
    }
  ]
}