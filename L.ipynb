{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "L.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOWX28F6z8bs4Kl+JZmg3aw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/layan121213/AASQ/blob/main/L.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZbCyJw25wWe",
        "outputId": "d1ce5351-e35e-4e8f-d0a1-582f00c64098"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import pandas as pd\n",
        "import pprint\n",
        "from gensim.models import word2vec , KeyedVectors\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import string\n",
        "import nltk \n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "from textblob import TextBlob\n",
        "import gensim\n",
        "from nltk.stem.lancaster import LancasterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "from F1 import evaluationMetric"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive"
      ],
      "metadata": {
        "id": "qxhpLjZXB0rE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BNF50kZB98r",
        "outputId": "92695f23-6ffe-41bc-a525-33c5f20c6a39"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/Colab Notebooks/GoogleNews-vectors-negative300.bin\" , binary=True)"
      ],
      "metadata": {
        "id": "4751nQGdCCxa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install gingerit"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 436
        },
        "id": "w8EqEoYlCGFY",
        "outputId": "dd365ec3-5100-423d-9d9e-7ac70e97b332"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gingerit\n",
            "  Downloading gingerit-0.8.2-py3-none-any.whl (3.3 kB)\n",
            "Collecting requests<3.0.0,>=2.25.1\n",
            "  Downloading requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "\u001b[?25l\r\u001b[K     |█████▏                          | 10 kB 31.4 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 20 kB 27.5 MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 30 kB 30.4 MB/s eta 0:00:01\r\u001b[K     |████████████████████▊           | 40 kB 25.1 MB/s eta 0:00:01\r\u001b[K     |██████████████████████████      | 51 kB 20.3 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 61 kB 23.1 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 63 kB 1.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2021.10.8)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2.10)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.25.1->gingerit) (2.0.12)\n",
            "Installing collected packages: requests, gingerit\n",
            "  Attempting uninstall: requests\n",
            "    Found existing installation: requests 2.23.0\n",
            "    Uninstalling requests-2.23.0:\n",
            "      Successfully uninstalled requests-2.23.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.27.1 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed gingerit-0.8.2 requests-2.27.1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "requests"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import cloudscraper\n",
        "# session = cloudscraper.create_scraper()"
      ],
      "metadata": {
        "id": "VxOk36T0CTN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cloudscraper"
      ],
      "metadata": {
        "id": "Ba1UfJDhCI05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import SnowballStemmer\n",
        "\n",
        "lancaster = LancasterStemmer()\n",
        "\n",
        "\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "def stem_words(text):\n",
        "    word_tokens = word_tokenize(text)\n",
        "    stems = [stemmer.stem(word) for word in word_tokens]\n",
        "    return ' '.join(stems)\n",
        "\n",
        "def stemming(text):\n",
        "    \n",
        "    snowball = SnowballStemmer(language='english')\n",
        "    \n",
        "    list=[]\n",
        "    for token in word_tokenize(text):\n",
        "        list.append(snowball.stem(token))\n",
        "    \n",
        "    return ' '.join(list)\n",
        "\n",
        "def preprocessing(t):\n",
        "    # print(word_tokenize(t))\n",
        "    t = str(t).lower()  # lowercasing\n",
        "    no_punc = t.translate(str.maketrans(\n",
        "        '', '', string.punctuation))  # punc removal\n",
        "    # stem = stem_words(no_punc)\n",
        "    # print(\"stem\",stem)\n",
        "    no_stopWords = stopwords.words('english')  # stop words removal\n",
        "    text_tokens = word_tokenize(no_punc)\n",
        "    not_stopword = [word for word in text_tokens if not word in no_stopWords]\n",
        "    # print(\"not_stopword\",not_stopword)\n",
        "    return not_stopword\n",
        "\n",
        "\n",
        "def clean(text):\n",
        "    cleaned = ' '.join([str(i) for i in text]).replace('[\\'', ' ').replace('\\']', '')\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "def listToString(s): \n",
        "    str1 = \"\" \n",
        "    for ele in s: \n",
        "        str1 += ele  \n",
        "    return str1 \n",
        "\n",
        "w2v_vocab = set(model.vocab)\n",
        "index2word_set = set(model.wv.index2word)\n",
        "\n",
        "def avg_feature_vector(sentence, model, num_features, index2word_set):\n",
        "    words = sentence.split()\n",
        "    feature_vec = np.zeros((num_features, ), dtype='float32')\n",
        "    n_words = 0\n",
        "    for word in words:\n",
        "        if word in index2word_set:\n",
        "            n_words += 1\n",
        "            feature_vec = np.add(feature_vec, model[word])\n",
        "    if (n_words > 0):\n",
        "        feature_vec = np.divide(feature_vec, n_words)\n",
        "    return feature_vec"
      ],
      "metadata": {
        "id": "-ON93KLTCKBu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scipy import spatial\n",
        "# from spellchecker import SpellChecker\n",
        "import math\n",
        "# from autocorrect import Speller\n",
        "from scipy.sparse import csr_matrix\n",
        "from numpy import count_nonzero\n",
        "import cloudscraper\n",
        "from gingerit.gingerit import GingerIt\n",
        "\n",
        "parser = GingerIt()\n",
        "# check = Speller(lang='en')\n",
        "# spell = SpellChecker()\n",
        "# check = Speller(lang='en')\n",
        "text = \"An algoriithm is a sequence of steps or a plan to complete an action or do something.\"\n",
        "ModwlA= parser.parse(text)['result']\n",
        "toString2 = clean(preprocessing(ModwlA))\n",
        "df = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/test.xlsx\", usecols=\"A\", index_col=None)\n",
        "a = pd.read_excel(\"/content/drive/MyDrive/Colab Notebooks/test.xlsx\", usecols=\"B\", index_col=None)\n",
        "count =0\n",
        "jacc = 0\n",
        "cos = 0.0\n",
        "div = 0\n",
        "list = []  \n",
        "val = []\n",
        "\n",
        "\n",
        "for rowIndex, row in df.iterrows(): \n",
        "    for value in row.items():\n",
        "        # print(\"res>>>>>\",value[1])\n",
        "        res=parser.parse(value[1])['result']\n",
        "        # print(\"gingerit:\",res)\n",
        "        toString = clean(preprocessing(res))\n",
        "        # print(\"Pre>>>>>\",toString)\n",
        "        s1_afv=avg_feature_vector(toString2, model=model, num_features=300, index2word_set=index2word_set)\n",
        "        S = csr_matrix(s1_afv)\n",
        "        B = S.todense()\n",
        "        s2_afv = avg_feature_vector(toString, model=model, num_features=300, index2word_set=index2word_set)\n",
        "        L = csr_matrix(s2_afv)\n",
        "        G = L.todense()\n",
        "        sim = 1 - spatial.distance.cosine(B, G)\n",
        "        if( math.isnan(sim)):\n",
        "          print(toString)\n",
        "          sim=0.0\n",
        "          print(sim)\n",
        "          val.append(sim)\n",
        "        else:\n",
        "          print(toString)\n",
        "          print(sim)\n",
        "          val.append(sim)\n",
        "\n",
        "        # cos += sim\n",
        "        # print(toString)\n",
        "        # print(sim)\n",
        "        # print(\"cos\",count)\n",
        "        \n",
        "predicted = [float(x) for x in val]\n",
        "actual = [int(x) for x in a.values] \n",
        "f1 = evaluationMetric(actual,predicted)\n",
        "print(f1)\n",
        "# print(index2word_set)\n",
        "# sparsity = 1.0 - count_nonzero(s1_afv)/ s1_afv.size\n",
        "# print(\"s1_afv\", sparsity)\n",
        "# sparsity2 = 1.0 - count_nonzero(s2_afv)/ s2_afv.size\n",
        "# print(\"s2_afv\", sparsity2)\n",
        "# print(\"\\njac average: \"+str(jacc/count))\n",
        "# print(\"average: \"+str(cos/count))\n",
        "# print(\"div average: \"+str(div/count))"
      ],
      "metadata": {
        "id": "OCamHvJJCV6Q"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}